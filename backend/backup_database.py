"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python backup_database.py
"""

import os
import subprocess
import logging
import hashlib
import json
from datetime import datetime, timedelta
from pathlib import Path
import boto3
from botocore.exceptions import ClientError

from app.config import settings

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class DatabaseBackup:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –∫–æ–ø–∏—è–º–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self):
        self.backup_dir = Path("backups")
        self.backup_dir.mkdir(exist_ok=True)
        self.s3_enabled = all([settings.AWS_ACCESS_KEY_ID, settings.AWS_SECRET_ACCESS_KEY, settings.AWS_S3_BUCKET])
        self.backup_metadata_file = self.backup_dir / "backup_metadata.json"

        if self.s3_enabled:
            self.s3_client = boto3.client(
                "s3",
                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
                region_name=getattr(settings, "AWS_REGION", "eu-west-1"),
            )
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ backup'–æ–≤
        self.backup_metadata = self._load_backup_metadata()

    def create_backup(self) -> Path:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_dir / f"mentorhub_backup_{timestamp}.sql"

        logger.info(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_file}")

        # –ü–∞—Ä—Å–∏–Ω–≥ DATABASE_URL
        db_url = settings.DATABASE_URL

        if db_url.startswith("sqlite"):
            # SQLite backup
            db_path = db_url.replace("sqlite:///", "")
            import shutil

            shutil.copy2(db_path, backup_file.with_suffix(".db"))
            backup_file = backup_file.with_suffix(".db")
            logger.info("‚úÖ SQLite –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞")

        elif db_url.startswith("postgresql"):
            # PostgreSQL backup
            try:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
                from urllib.parse import urlparse

                parsed = urlparse(db_url)

                env = os.environ.copy()
                env["PGPASSWORD"] = parsed.password or ""

                cmd = [
                    "pg_dump",
                    "-h",
                    parsed.hostname or "localhost",
                    "-p",
                    str(parsed.port or 5432),
                    "-U",
                    parsed.username or "postgres",
                    "-d",
                    parsed.path.lstrip("/"),
                    "-F",
                    "c",  # Custom format (compressed)
                    "-f",
                    str(backup_file),
                ]

                subprocess.run(cmd, check=True, env=env)
                logger.info("‚úÖ PostgreSQL –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞")

            except subprocess.CalledProcessError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è backup: {e}")
                raise
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                raise

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {db_url}")

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        size_mb = backup_file.stat().st_size / (1024 * 1024)
        logger.info(f"üìä –†–∞–∑–º–µ—Ä backup: {size_mb:.2f} MB")

        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        file_hash = self._calculate_file_hash(backup_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._save_backup_metadata(backup_file.name, size_mb, file_hash, "full")

        return backup_file

    def upload_to_s3(self, backup_file: Path) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ backup –≤ S3"""
        if not self.s3_enabled:
            logger.warning("‚ö†Ô∏è S3 –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏")
            return False

        try:
            s3_key = f"backups/{backup_file.name}"

            logger.info(f"‚òÅÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3: s3://{settings.AWS_S3_BUCKET}/{s3_key}")

            self.s3_client.upload_file(
                str(backup_file),
                settings.AWS_S3_BUCKET,
                s3_key,
                ExtraArgs={
                    "ServerSideEncryption": "AES256",
                    "StorageClass": "STANDARD_IA",  # Infrequent Access (–¥–µ—à–µ–≤–ª–µ)
                },
            )

            logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ S3")
            return True

        except ClientError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ S3: {e}")
            return False

    def cleanup_old_backups(self, keep_days: int = 7):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö backup'–æ–≤"""
        logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ backup'–æ–≤ —Å—Ç–∞—Ä—à–µ {keep_days} –¥–Ω–µ–π")

        cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 60 * 60)
        deleted_count = 0

        for backup_file in self.backup_dir.glob("mentorhub_backup_*"):
            if backup_file.stat().st_mtime < cutoff_time:
                backup_file.unlink()
                deleted_count += 1
                logger.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω: {backup_file.name}")

        logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä—ã—Ö backup'–æ–≤")
        
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
        
    def _save_backup_metadata(self, filename: str, size_mb: float, file_hash: str, backup_type: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö backup'–∞"""
        self.backup_metadata[filename] = {
            "timestamp": datetime.now().isoformat(),
            "size_mb": size_mb,
            "hash": file_hash,
            "type": backup_type
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        with open(self.backup_metadata_file, 'w') as f:
            json.dump(self.backup_metadata, f, indent=2, default=str)
            
    def _load_backup_metadata(self) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö backup'–æ–≤"""
        if self.backup_metadata_file.exists():
            try:
                with open(self.backup_metadata_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
                return {}
        return {}
        
    def get_backup_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ backup'–∞—Ö"""
        info = {
            "total_backups": len(self.backup_metadata),
            "backup_types": {},
            "recent_backups": []
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã backup'–æ–≤
        type_counts = {}
        for metadata in self.backup_metadata.values():
            backup_type = metadata.get("type", "unknown")
            type_counts[backup_type] = type_counts.get(backup_type, 0) + 1
        info["backup_types"] = type_counts
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 backup'–æ–≤
        sorted_backups = sorted(
            self.backup_metadata.items(), 
            key=lambda x: x[1].get("timestamp", ""), 
            reverse=True
        )[:10]
        
        info["recent_backups"] = [
            {"filename": name, "metadata": metadata} 
            for name, metadata in sorted_backups
        ]
        
        return info

    def restore_backup(self, backup_file: Path):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
        logger.warning(f"‚ö†Ô∏è –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ó BACKUP: {backup_file}")
        logger.warning("‚ö†Ô∏è –í—Å–µ —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –ü–ï–†–ï–ó–ê–ü–ò–°–ê–ù–´!")

        if not backup_file.exists():
            raise FileNotFoundError(f"Backup —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")

        db_url = settings.DATABASE_URL

        if db_url.startswith("sqlite"):
            # SQLite restore
            db_path = db_url.replace("sqlite:///", "")
            import shutil

            # –°–æ–∑–¥–∞–Ω–∏–µ backup —Ç–µ–∫—É—â–µ–π –ë–î
            current_backup = f"{db_path}.before_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.copy2(db_path, current_backup)
            logger.info(f"üíæ –¢–µ–∫—É—â–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {current_backup}")

            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
            shutil.copy2(backup_file, db_path)
            logger.info("‚úÖ SQLite –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

        elif db_url.startswith("postgresql"):
            # PostgreSQL restore
            from urllib.parse import urlparse

            parsed = urlparse(db_url)

            env = os.environ.copy()
            env["PGPASSWORD"] = parsed.password or ""

            cmd = [
                "pg_restore",
                "-h",
                parsed.hostname or "localhost",
                "-p",
                str(parsed.port or 5432),
                "-U",
                parsed.username or "postgres",
                "-d",
                parsed.path.lstrip("/"),
                "-c",  # Clean (drop) database objects before recreating
                "-F",
                "c",  # Custom format
                str(backup_file),
            ]

            try:
                subprocess.run(cmd, check=True, env=env)
                logger.info("‚úÖ PostgreSQL –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            except subprocess.CalledProcessError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
                raise

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {db_url}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ backup"""
    backup = DatabaseBackup()

    try:
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ backup
        backup_file = backup.create_backup()

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3 (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)
        backup.upload_to_s3(backup_file)

        # 3. –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö backup'–æ–≤
        backup.cleanup_old_backups(keep_days=7)

        logger.info("üéâ Backup —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω!")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è backup: {e}")
        raise


if __name__ == "__main__":
    main()
